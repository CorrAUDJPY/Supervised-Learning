{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensemble Methods and Spam Detection",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6WuOAyYPRGniR96qC7QzG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CorrAUDJPY/Supervised-Learning/blob/main/Ensemble_Methods_and_Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX043b0Miln0"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import   accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cNG3TRri0p0",
        "outputId": "6eb11a9a-6393-4733-d309-5a583d2c8add"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "curr_dir='/content/drive/My Drive/Colab Notebooks/Udacity_Supervised_Learning/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yk03YvGGjGVg"
      },
      "source": [
        "df=pd.read_table( curr_dir+'SMSSpamCollection',\n",
        "                 sep='\\t',\n",
        "                 header=None,\n",
        "                 names=['label','sms_message'])\n",
        "\n",
        "df['label'] = df.label.map( {'ham':0, 'spam':1})\n",
        "\n",
        "X_train, X_test, y_train, y_test =train_test_split( df['sms_message'] , df['label'] , random_state=1)\n"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AoB2pj4NkAI1",
        "outputId": "43ed0bd4-9904-478b-f91d-7380ae62854f"
      },
      "source": [
        "print( '\\n df.head \\n' , df.head(2) )\n",
        "print( '\\n df.label \\n', df['label'].head(2) )\n",
        "\n",
        "print( '\\n df.sms \\n', df['sms_message'].head(2) )\n",
        "\n",
        "print( '\\n Xtrain \\n', X_train.head(2) )\n",
        "print( '\\n Ytrain \\n', y_train.head(2) )\n",
        "\n",
        "print( '\\n Xtest \\n', X_test.head(2) )\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " df.head \n",
            "    label                                        sms_message\n",
            "0      0  Go until jurong point, crazy.. Available only ...\n",
            "1      0                      Ok lar... Joking wif u oni...\n",
            "\n",
            " df.label \n",
            " 0    0\n",
            "1    0\n",
            "Name: label, dtype: int64\n",
            "\n",
            " df.sms \n",
            " 0    Go until jurong point, crazy.. Available only ...\n",
            "1                        Ok lar... Joking wif u oni...\n",
            "Name: sms_message, dtype: object\n",
            "\n",
            " Xtrain \n",
            " 710     4mths half price Orange line rental & latest c...\n",
            "3740                           Did you stitch his trouser\n",
            "Name: sms_message, dtype: object\n",
            "\n",
            " Ytrain \n",
            " 710     1\n",
            "3740    0\n",
            "Name: label, dtype: int64\n",
            "\n",
            " Xtest \n",
            " 1078                     Yep, by the pretty sculpture\n",
            "4028    Yes, princess. Are you going to make me moan?\n",
            "Name: sms_message, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpQDQh_Ypqo1",
        "outputId": "84683935-5d53-4e85-c236-2ab0caac9ca8"
      },
      "source": [
        "# Instantiate the CountVectorizer method\n",
        "count_vector = CountVectorizer()\n",
        "\n",
        "# Fit the training data and then return the matrix\n",
        "training_data = count_vector.fit_transform(X_train)\n",
        "\n",
        "# Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()\n",
        "testing_data = count_vector.transform(X_test)\n",
        "\n",
        "# Instantiate our model\n",
        "naive_bayes = MultinomialNB()\n",
        "\n",
        "# Fit our model to the training data\n",
        "naive_bayes.fit(training_data, y_train)\n",
        "\n",
        "predictions= naive_bayes.predict( testing_data)\n",
        "accuracy=accuracy_score( y_test ,  predictions)\n",
        "precision=precision_score( y_test ,  predictions)\n",
        "recall=recall_score( y_test ,  predictions)\n",
        "f1=f1_score( y_test ,  predictions)\n",
        "\n",
        "\n",
        "print( '\\n Accur={} , Prec={} , Recall={} , f1={}'.format(accuracy , precision , recall , f1)  )\n",
        "\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Accur=0.9885139985642498 , Prec=0.9720670391061452 , Recall=0.9405405405405406 , f1=0.9560439560439562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLc_9rcEpfT7",
        "outputId": "1d6d7a9c-aa0e-49c4-ec3d-298d68e8818b"
      },
      "source": [
        "print(type(predictions))\n",
        "\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NfMqIBAy0ok"
      },
      "source": [
        "Turns Out...\n",
        "It turns out that our naive bayes model actually does a pretty good job. However, let's take a look at a few additional models to see if we can't improve anyway.\n",
        "\n",
        "Specifically in this notebook, we will take a look at the following techniques:\n",
        "\n",
        "*   BaggingClassifier\n",
        "*   RandomForestClassifier\n",
        "*   AdaBoostClassifier\n",
        "\n",
        "These ***ensemble*** methods use a combination of techniques you have seen throughout this lesson:\n",
        "\n",
        "***BOOSTING***: Train  'weak' classifiers ***SEQUENTIALLY***.  The weak classifiers are weighted according to their accuracy. After each iteration, the training is re-weighted so that misclassified instances get higher weight, and correctly classified instances get lower weight.\n",
        "Boosting algorithms include: AdaBoost, GBP, XGBM, Light GBM, CatBoost.\n",
        "Boosting algorithms likely tend to ***OVERFIT***.\n",
        "\n",
        "\n",
        "\n",
        "***BAGGING***: Train  'weak' classifiers ***IN-PARALEL***.  Fit different classifiers in parallel o different training set subsets.  Then combine them.  Bagging tends to ***REDUCE VARIANCE***.\n",
        "\n",
        "Bootstrap the data passed through a learner (bagging).\n",
        "Subset the features used for a learner (combined with bagging signifies the two random components of random forests).\n",
        "Ensemble learners together in a way that allows those that perform best in certain areas to create the largest impact (boosting).\n",
        "In this notebook, let's get some practice with these methods, which will also help you get comfortable with the process used for performing supervised machine learning in python in general.\n",
        "\n",
        "Since you cleaned and vectorized the text in the previous notebook, this notebook can be focused on the fun part - the machine learning part.\n",
        "\n",
        "This Process Looks Familiar...\n",
        "In general, there is a five step process that can be used each type you want to use a supervised learning method (which you actually used above):\n",
        "\n",
        "Import the model.\n",
        "Instantiate the model with the hyperparameters of interest.\n",
        "Fit the model to the training data.\n",
        "Predict on the test data.\n",
        "Score the model by comparing the predictions to the actual values.\n",
        "Follow the steps through this notebook to perform these steps using each of the ensemble methods: BaggingClassifier, RandomForestClassifier, and AdaBoostClassifier.\n",
        "\n",
        "Step 1: First use the documentation to import all three of the models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-dVuKW7y8G3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}